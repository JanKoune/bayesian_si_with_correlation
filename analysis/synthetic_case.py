"""
Bayesian system identification for the IJsselbridge

This file is part of the analyses for the journal publication based on the
thesis: "Bayesian system identification for structures considering spatial
and temporal dependencies"
"""

# =========================================================================
# IMPORT JULIA
#
# When julia can be added to the PATH environment variable, we can use the
# high level API. Otherwise we must specify the executable. This causes a dll
# not found error which can be fixed by directly loading the dll. Additionally,
# PyCall will also sometimes fail the first time its called, hence the try-except
# ===========================================================================
from modules import paths
from julia import Julia

julia_bin_path = paths["julia"]
libpaths = ["libgomp-1.dll", "libpcre2-8-0.dll"]
try:
    jl = Julia(runtime=julia_bin_path + "julia.exe")
except:
    jl = Julia(runtime=julia_bin_path + "julia.exe")
jl.eval("using Libdl")
for libname in libpaths:
    libdl_eval_string = "Libdl.dlopen(" + '"' + julia_bin_path + libname + '"' ")"
    jl.eval(libdl_eval_string)
jl.eval("using LinearAlgebra")
from julia import Main
# ===========================================================================
# IMPORTS
# ===========================================================================

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from taralli.parameter_estimation.base import DynestyParameterEstimator

import torch
from torch.distributions import MultivariateNormal

from scipy import stats

from tripy.loglikelihood import chol_loglike_2D, kron_loglike_2D
from tripy.utils import inv_cov_vec_1D

from modules.IJssel_bridge_truck_data import *
from modules.IJssel_bridge_plotting import (
    plot_credible_intervals,
    update_plot_params,
    reset_global_default,
    paramsPostPred,
    plot_post_pred,
    plot_posterior,
)
from models.IJssel_bridge_model_twin_girder_betti import IJssel_bridge_model

from modules.IJssel_bridge_utils import (
    summary_model_selection,
    paths,
    save_model,
    save_post_pred,
    load_pickle,
    dynesty_resample,
    model_selection,
    get_theta_from_input,
    func_sample_noise,
    sample_post_pred,
    get_parameter_estimates
)

from modules.IJssel_bridge_corr_utils import (
    Independence,
    Exponential,
    RBF,
    RationalQuadratic,
    DampedCosine,
    Matern,
    iid_loglike_2D_multiplicative,
    iid_loglike_2D_additive
)

from pathlib import Path

# Silence julia Futurewarning and numpy deprecation warning related to the posterior
# plot. The Julia warning is printed every time there is console output
import os
import warnings

warnings.simplefilter(action="ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=np.VisibleDeprecationWarning)

# ============================================================================
# SETTINGS
# ============================================================================
# Data
truck_path = "leftright"  # Which truck path to run inference for
meas_type = "fugro"  # Use TNO or fugro measurements and model setup

# Dynest
estimation_method = "static"
maxcall = 100_000
nlive = 200
sample = "auto"
bound = "multi"

# General
load_data_flag = 0  # Load posterior samples from previous run
save_model_flag = 0  # Save first run data (no effect if load_data_flag == 1)
save_results_flag = 0  # Save the set of samples drawn from the posterior predictives
save_meas_flag = 1 # Save synthetic measurements
plot_res_flag = 0  # Plots for first run with n_pts = nnode
post_pred_flag = 0  # Calculate posterior predictive
load_results_flag = 0   # Load posterior predictive samples calculated in previous run
plot_samples_peak_flag = 0
n_samples_ci = 5000
n_samples_post_pred = 10
prob_mass = 0.9

# ================================================================
# CREATE DIRECTORIES
# ================================================================
scriptname = os.path.basename(__file__).split(".")[0]
fig_post_save_path = Path(paths["figures"]) / "autogenerated" / scriptname / "posteriors"
fig_valid_save_path = Path(paths["figures"]) / "autogenerated" / scriptname / "validation"
fig_pred_save_path = Path(paths["figures"]) / "autogenerated" / scriptname / "prediction"
data_save_path = Path(paths["data"]) / "autogenerated" / scriptname
results_save_path = Path(paths["results"]) / "autogenerated" / scriptname
meas_save_path = Path(paths["measurements"]) / "autogenerated" / scriptname
fig_post_save_path.mkdir(parents=True, exist_ok=True)
fig_valid_save_path.mkdir(parents=True, exist_ok=True)
fig_pred_save_path.mkdir(parents=True, exist_ok=True)
data_save_path.mkdir(parents=True, exist_ok=True)
results_save_path.mkdir(parents=True, exist_ok=True)


# ================================================================
# Functions
# ================================================================
def get_parameter_idx(params, model_params, allow_none=False):
    # This function causing an error if the requested parameter
    # is not found is intentionall. Preferable than returning an
    # empty list or fewer elements than expected, which could
    # have unintended results
    param_idx = []
    for param in params:
        if param in model_params:
            param_idx.append(model_params.index(param))
        elif (param not in model_params) and (allow_none == True):
            param_idx.append(None)
        elif (param not in model_params) and (allow_none == False):
            raise ValueError(f"Parameter {param} not found in model model parameters")
    return param_idx


log_transform = lambda x: np.log10(x)


def get_sensors_per_span(start, end, N):
    if not ((end - start) > 0.0):
        raise ValueError("End value must be larger than start value")
    if not N > 0:
        raise ValueError("N must be larger than 0")
    if not type(N) == int:
        raise ValueError("N must be an integer")
    interval_length = (end - start) / (N + 1)
    pos = [start + interval_length * i for i in range(1, N + 1)]
    return pos


# ================================================================
# MAIN ANALYSIS LOOP
#
# NOTE: The sensors used to perform inference are defined as a list
# of lists. Each inner lists contains a single sensor that will be
# used to perform the inference. If multiple sensors are defined in
# an inner list, inference will be performed using all of them.
# ================================================================

# Set parameters
E = 210e6
max_elem_length = 2.0 * 1e3
N_out = 2
Niter = 20

# Reference x vector. Node numbers and locations may vary per model
# so all physical model outputs are interpolated to the reference model nodes.
ref_model = IJssel_bridge_model(
    65.37, E, max_elem_length
)

# List of sensors for inference
Nsens_per_span = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
span_intervals = np.vstack((ref_model.support_xs[:-1], ref_model.support_xs[1:])).T
Nspans = len(span_intervals)

# Ground truth parameter values
dict_gt = {
    "Cv": 0.15,
    "std_model": 2.5,
    "std_meas": 0.8,
    "lcorr_x": 50.0,
    "lcorr_RBF": 75.0,
    "lcorr_EXP": 75.0,
    "Kv": 3.2,
    "Kr1": 6.5,
    "Kr2": 6.0,
    "Kr3": 5.5,
    "Kr4": 7.0,
}

# # Ground truth model
# gt_model = "IID_m_noise"
# gt_model = "IID_a"
# gt_model = "RBF_m_noise"
# gt_model = "RBF_a_noise"
# gt_model = "EXP_m_noise"
gt_model = "EXP_a_noise"

# Priors
Kr0_low = 4.0
Kr0_high = 10.0
Kv0_low = 0.0
Kv0_high = 8.0
cov_model_high = 1.0
std_model_high = 5.0
std_meas_high = 5.0
std_meas = np.finfo(float).eps
c0 = -0.17543859649122806
Cv_x = 1.0

param_estimates_list = []
model_selection_results_list = []
model_selection_latex_list = []
stat_models_list = []
for iter_i in range(Niter):
# for iter_i in range(1):
    for idx_Nspan, Nspan_i in enumerate(Nsens_per_span):

        Nx = Nspan_i * Nspans
        Nt = Nx

        # Names and locations of sensors considered in the current iteration
        fnames = [f"S{i + 1}" for i in range(Nx)]
        sensor_x = list(np.ravel([get_sensors_per_span(span_i[0], span_i[1], int(Nspan_i)) for span_i in span_intervals]))

        print("================================================================")
        print(f"Running iteration {idx_Nspan+1} with {Nspan_i} sensors per span")
        print(f"Sensor names = {fnames}")
        print(f"Sensor X positions = {sensor_x}")
        print("----------------------------------------------------------------")

        # ===========================================================================
        # MODEL SETUP
        # ===========================================================================

        # Define a model for each sensor. The generated models do not have the same
        # nodes at the same positions. Interpolation is needed.
        phys_model_list = []
        for i, name in enumerate(fnames):
            # Get sensor position

            # Define model
            phys_model_list.append(
                IJssel_bridge_model(
                    sensor_x[i],
                    E,
                    max_elem_length,
                    truck_load=meas_type
                )
            )

        n_phys_models = len(phys_model_list)
        node_ref_min = np.min(ref_model.node_xs)
        node_ref_max = np.max(ref_model.node_xs)

        # # Equidistant observations in time
        # node_xs = np.linspace(node_ref_min, node_ref_max, Nt)

        # # Position in time coinciding with sensor positions in space
        node_xs = np.array(sensor_x.copy())

        # Ground truth model used to generate the dataset and called during the Bayesian inference
        def phys_model_gt(t):
            Kv = dict_gt["Kv"]
            Kr = [dict_gt["Kr1"], dict_gt["Kr2"], dict_gt["Kr3"], dict_gt["Kr4"]]
            Kv = 10 ** Kv
            Kr = np.repeat(np.array(Kr), 2)
            Kr = np.append(Kr, np.zeros(4))
            Kr = 10 ** Kr
            y_left = np.zeros((n_phys_models, len(t)))
            y_right = np.zeros((n_phys_models, len(t)))
            for jj, ph_model in enumerate(phys_model_list):
                y_left[jj, :] = np.interp(
                    t,
                    ph_model.node_xs,
                    ph_model.il_stress_truckload(c0, "left", Kr=Kr, Kv=Kv)
                )
                y_right[jj, :] = np.interp(
                    t,
                    ph_model.node_xs,
                    ph_model.il_stress_truckload(c0, "right", Kr=Kr, Kv=Kv)
                )
            return np.vstack((y_left, y_right))

        # Get ground truth and time coordinates of points where inference is
        # performed.
        coords_infer = np.reshape(node_xs, (-1, 1))
        y_ground_truth_infer = phys_model_gt(node_xs)
        y_l_gt = y_ground_truth_infer[:Nx, :]
        y_r_gt = y_ground_truth_infer[Nx:, :]

        # Get location of peak for each influence line and corresponding
        # ground truth prediction. These are the locations where the posterior
        # predictive is calculated
        idx_x_pred = np.sort(np.argmax(y_r_gt, axis=1))
        x_pred = np.sort(node_xs[idx_x_pred])
        coords_pred = x_pred.reshape(-1, 1)
        y_ground_truth_pred = y_ground_truth_infer[:, idx_x_pred]

        # # Vizualize space-time grid of measurements used in inference
        # if iter_i == 0:
        #     X_plot, T_plot = np.meshgrid(sensor_x, node_xs)
        #     fig, ax = plt.subplots()
        #     ax.set_aspect(1)
        #     ax.scatter(X_plot, T_plot, c="black")
        #     ax.set_xlim(ref_model.node_xs[0] - 20.0, ref_model.node_xs[-1] + 20.0)
        #     ax.set_ylim(ref_model.node_xs[0] - 20.0, ref_model.node_xs[-1] + 20.0)
        #     ax.set_xlabel("Sensor x-position [m]")
        #     ax.set_ylabel("Truck x-position [m]")
        #     fig.suptitle(f"N = {Nspan_i}")
        #     plt.show()

        def phys_model_infer():
            return y_ground_truth_infer

        def phys_model_pred():
            return y_ground_truth_pred

        # ==========================================================================
        # LOG PRIORS
        # ============================================================================

        model_groups = {
            "multiplicative": [
                "IID_m_noise",
                "RBF_m_noise",
                "EXP_m_noise",
            ],
            "additive": [
                "IID_a",
                "RBF_a_noise",
                "EXP_a_noise",
            ]
        }

        models_run = [
            "IID_m_noise",
            "RBF_m_noise",
            "EXP_m_noise",
            "IID_a",
            "RBF_a_noise",
            "EXP_a_noise",
        ]

        models_evidence = [
            "IID_m_noise",
            "RBF_m_noise",
            "EXP_m_noise",
            "IID_a",
            "RBF_a_noise",
            "EXP_a_noise",
        ]

        model_latex_labels = {
            "IID_m_noise": r"IID-M",
            "RBF_m_noise": r"RBF-M",
            "EXP_m_noise": r"EXP-M",
            "REF_m_noise": r"REF-M",
            "IID_a": r"IID-A",
            "RBF_a_noise": r"RBF-A",
            "EXP_a_noise": r"EXP-A",
            "REF_a": r"REF-A",
        }

        params_plot = {
            "multiplicative": ["Cv"],
            "additive": ["std_model"]
        }

        # Assemble list of models
        models = []
        for key in model_groups.keys():
            for item in model_groups[key]:
                models.append(item)
        N_models = len(models)

        # Initialize kernels
        kernels_infer = {
            "IID_m": Independence(coords_infer),
            "RBF_m": RBF(coords_infer),
            "RQD_m": RationalQuadratic(coords_infer),
            "MAT_m": Matern(coords_infer),
            "EXP_m": Exponential(coords_infer),
            "COS_m": DampedCosine(coords_infer),
            "REF_m": Independence(coords_infer),
            "IID_m_noise": Independence(coords_infer),
            "RBF_m_noise": RBF(coords_infer),
            "RQD_m_noise": RationalQuadratic(coords_infer),
            "MAT_m_noise": Matern(coords_infer),
            "EXP_m_noise": Exponential(coords_infer),
            "COS_m_noise": DampedCosine(coords_infer),
            "REF_m_noise": Independence(coords_infer),
            "IID_a": Independence(coords_infer),
            "RBF_a": RBF(coords_infer),
            "RQD_a": RationalQuadratic(coords_infer),
            "MAT_a": Matern(coords_infer),
            "EXP_a": Exponential(coords_infer),
            "COS_a": DampedCosine(coords_infer),
            "REF_a": Independence(coords_infer),
            "IID_a_noise": Independence(coords_infer),
            "RBF_a_noise": RBF(coords_infer),
            "RQD_a_noise": RationalQuadratic(coords_infer),
            "MAT_a_noise": Matern(coords_infer),
            "EXP_a_noise": Exponential(coords_infer),
            "COS_a_noise": DampedCosine(coords_infer),
            "REF_a_noise": Independence(coords_infer),
            "X_dim": Exponential(np.reshape(sensor_x, (-1, 1)))
        }

        kernels_pred = {
            "IID_m": Independence(coords_pred),
            "RBF_m": RBF(coords_pred),
            "RQD_m": RationalQuadratic(coords_pred),
            "MAT_m": Matern(coords_pred),
            "EXP_m": Exponential(coords_pred),
            "COS_m": DampedCosine(coords_pred),
            "REF_m": Independence(coords_pred),
            "IID_m_noise": Independence(coords_pred),
            "RBF_m_noise": RBF(coords_pred),
            "RQD_m_noise": RationalQuadratic(coords_pred),
            "MAT_m_noise": Matern(coords_pred),
            "EXP_m_noise": Exponential(coords_pred),
            "COS_m_noise": DampedCosine(coords_pred),
            "REF_m_noise": Independence(coords_pred),
            "IID_a": Independence(coords_pred),
            "RBF_a": RBF(coords_pred),
            "RQD_a": RationalQuadratic(coords_pred),
            "MAT_a": Matern(coords_pred),
            "EXP_a": Exponential(coords_pred),
            "COS_a": DampedCosine(coords_pred),
            "REF_a": Independence(coords_pred),
            "IID_a_noise": Independence(coords_pred),
            "RBF_a_noise": RBF(coords_pred),
            "RQD_a_noise": RationalQuadratic(coords_pred),
            "MAT_a_noise": Matern(coords_pred),
            "EXP_a_noise": Exponential(coords_pred),
            "COS_a_noise": DampedCosine(coords_pred),
            "REF_a_noise": Independence(coords_pred),
            "X_dim": Exponential(np.reshape(sensor_x, (-1, 1)))
        }

        params_phys = {key: [] for key in models}

        params_unc = {
            "IID_m": ["Cv"],
            "RBF_m": ["Cv", "lcorr_RBF", "lcorr_x"],
            "MAT_m": ["Cv", "lcorr_MAT", "lcorr_x"],
            "EXP_m": ["Cv", "lcorr_EXP", "lcorr_x"],
            "REF_m": ["Cv"],
            "IID_m_noise": ["Cv", "std_meas"],
            "RBF_m_noise": ["Cv", "lcorr_RBF", "lcorr_x", "std_meas"],
            "MAT_m_noise": ["Cv", "lcorr_MAT", "lcorr_x", "std_meas"],
            "EXP_m_noise": ["Cv", "lcorr_EXP", "lcorr_x", "std_meas"],
            "REF_m_noise": ["Cv", "std_meas"],
            "IID_a": ["std_model"],
            "RBF_a": ["std_model", "lcorr_RBF", "lcorr_x"],
            "MAT_a": ["std_model", "lcorr_MAT", "lcorr_x"],
            "EXP_a": ["std_model", "lcorr_EXP", "lcorr_x"],
            "REF_a": ["std_model"],
            "IID_a_noise": ["std_model", "std_meas"],
            "RBF_a_noise": ["std_model", "lcorr_RBF", "lcorr_x", "std_meas"],
            "MAT_a_noise": ["std_model", "lcorr_MAT", "lcorr_x", "std_meas"],
            "EXP_a_noise": ["std_model", "lcorr_EXP", "lcorr_x", "std_meas"],
            "REF_a_noise": ["std_model", "std_meas"],
        }

        # Define transformations for plotting
        params_transforms = {
            "Cv": None,
            "std_model": None,
            "std_meas": None,
            "Kv": None,
            "lcorr_RBF": None,
            "lcorr_RQD": None,
            "lcorr_MAT": None,
            "lcorr_EXP": None,
            "lcorr_COS": None,
            "lcorr_x": None,
            "alpha": None,
            "wn": None,
            "Kr1": None,
            "Kr2": None,
            "Kr3": None,
            "Kr4": None,
        }

        params_latex_labels = {
            "Kv": r"$\mathrm{log_{10}}(K_{\mathrm{v}})$",
            "Cv": r"$C_{\mathrm{v}}$",
            "std_model": r"$\sigma_{\mathrm{model}}$",
            "std_meas": r"$\sigma_{\mathrm{meas}}$",
            "Kr1": r"$\mathrm{log_{10}}(K_{\mathrm{r,1}})$",
            "Kr2": r"$\mathrm{log_{10}}(K_{\mathrm{r,2}})$",
            "Kr3": r"$\mathrm{log_{10}}(K_{\mathrm{r,3}})$",
            "Kr4": r"$\mathrm{log_{10}}(K_{\mathrm{r,4}})$",
            "lcorr_RBF": r"$l_{\mathrm{corr,t}}$",
            "lcorr_RQD": r"$l_{\mathrm{corr,t}}$",
            "lcorr_MAT": r"$l_{\mathrm{corr,t}}$",
            "lcorr_EXP": r"$l_{\mathrm{corr,t}}$",
            "lcorr_COS": r"$l_{\mathrm{corr,t}}$",
            "lcorr_x": r"$l_{\mathrm{corr,x}}$",
            "alpha": r"$\alpha$",
            "wn": r"$w_n$",
        }

        params_priors = {
            "Kv": {"dist": "uniform", "low": Kv0_low, "high": Kv0_high},
            "Kr1": {"dist": "uniform", "low": Kr0_low, "high": Kr0_high},
            "Kr2": {"dist": "uniform", "low": Kr0_low, "high": Kr0_high},
            "Kr3": {"dist": "uniform", "low": Kr0_low, "high": Kr0_high},
            "Kr4": {"dist": "uniform", "low": Kr0_low, "high": Kr0_high},
            "Cv": {"dist": "uniform", "low": 0.01, "high": cov_model_high},
            "std_model": {"dist": "uniform", "low": 0.1, "high": std_model_high},
            "std_meas": {"dist": "uniform", "low": 0.1, "high": std_meas_high},
            "lcorr_RBF": {"dist": "uniform", "low": 0.1, "high": 200.0},
            "lcorr_RQD": {"dist": "uniform", "low": 0.1, "high": 200.0},
            "lcorr_MAT": {"dist": "uniform", "low": 0.1, "high": 200.0},
            "lcorr_EXP": {"dist": "uniform", "low": 0.1, "high": 200.0},
            "lcorr_COS": {"dist": "uniform", "low": 0.1, "high": 200.0},
            "alpha": {"dist": "uniform", "low": 0.0001, "high": 5.0},
            "wn": {"dist": "uniform", "low": 0.0000001, "high": 0.5},
            "lcorr_x": {"dist": "uniform", "low": 0.01, "high": 200.0},
        }

        # There is definitely a better way to do this
        kernel_funcs_infer = {
            "IID_m": lambda theta: kernels_infer["IID_m"].forward(theta["Cv"]),
            "RBF_m": lambda theta: kernels_infer["RBF_m"].forward(theta["Cv"], length_scale=theta["lcorr_RBF"]),
            "RQD_m": lambda theta: kernels_infer["RQD_m"].forward(theta["Cv"], length_scale=theta["lcorr_RQD"],
                                                                  alpha=theta["alpha"]),
            "MAT_m": lambda theta: kernels_infer["MAT_m"].forward(theta["Cv"], length_scale=theta["lcorr_MAT"], nu=1.5),
            "EXP_m": lambda theta: kernels_infer["EXP_m"].forward(theta["Cv"], length_scale=theta["lcorr_EXP"]),
            "COS_m": lambda theta: kernels_infer["COS_m"].forward(theta["Cv"], length_scale=theta["lcorr_COS"],
                                                                  wn=theta["wn"]),
            "REF_m": lambda theta: kernels_infer["REF_m"].forward(theta["Cv"]),
            "IID_m_noise": lambda theta: kernels_infer["IID_m_noise"].forward(theta["Cv"]),
            "RBF_m_noise": lambda theta: kernels_infer["RBF_m_noise"].forward(theta["Cv"], length_scale=theta["lcorr_RBF"]),
            "RQD_m_noise": lambda theta: kernels_infer["RQD_m_noise"].forward(theta["Cv"], length_scale=theta["lcorr_RQD"],
                                                                              alpha=theta["alpha"]),
            "MAT_m_noise": lambda theta: kernels_infer["MAT_m_noise"].forward(theta["Cv"], length_scale=theta["lcorr_MAT"],
                                                                              nu=1.5),
            "EXP_m_noise": lambda theta: kernels_infer["EXP_m_noise"].forward(theta["Cv"], length_scale=theta["lcorr_EXP"]),
            "COS_m_noise": lambda theta: kernels_infer["COS_m_noise"].forward(theta["Cv"], length_scale=theta["lcorr_COS"],
                                                                              wn=theta["wn"]),
            "REF_m_noise": lambda theta: kernels_infer["REF_m_noise"].forward(theta["Cv"]),
            "IID_a": lambda theta: kernels_infer["IID_a"].forward(theta["std_model"]),
            "RBF_a": lambda theta: kernels_infer["RBF_a"].forward(theta["std_model"], length_scale=theta["lcorr_RBF"]),
            "RQD_a": lambda theta: kernels_infer["RQD_a"].forward(theta["std_model"], length_scale=theta["lcorr_RQD"],
                                                                  alpha=theta["alpha"]),
            "MAT_a": lambda theta: kernels_infer["MAT_a"].forward(theta["std_model"], length_scale=theta["lcorr_MAT"],
                                                                  nu=1.5),
            "EXP_a": lambda theta: kernels_infer["EXP_a"].forward(theta["std_model"], length_scale=theta["lcorr_EXP"]),
            "COS_a": lambda theta: kernels_infer["COS_a"].forward(theta["std_model"], length_scale=theta["lcorr_COS"],
                                                                  wn=theta["wn"]),
            "REF_a": lambda theta: kernels_infer["REF_a"].forward(theta["std_model"]),
            "IID_a_noise": lambda theta: kernels_infer["IID_a_noise"].forward(theta["std_model"]),
            "RBF_a_noise": lambda theta: kernels_infer["RBF_a_noise"].forward(theta["std_model"],
                                                                              length_scale=theta["lcorr_RBF"]),
            "RQD_a_noise": lambda theta: kernels_infer["RQD_a_noise"].forward(theta["std_model"],
                                                                              length_scale=theta["lcorr_RQD"],
                                                                              alpha=theta["alpha"]),
            "MAT_a_noise": lambda theta: kernels_infer["MAT_a_noise"].forward(theta["std_model"],
                                                                              length_scale=theta["lcorr_MAT"], nu=1.5),
            "EXP_a_noise": lambda theta: kernels_infer["EXP_a_noise"].forward(theta["std_model"],
                                                                              length_scale=theta["lcorr_EXP"]),
            "COS_a_noise": lambda theta: kernels_infer["COS_a_noise"].forward(theta["std_model"],
                                                                              length_scale=theta["lcorr_COS"],
                                                                              wn=theta["wn"]),
            "REF_a_noise": lambda theta: kernels_infer["REF_a_noise"].forward(theta["std_model"]),
            "X_dim": lambda theta: kernels_infer["X_dim"].forward(Cv_x, length_scale=theta["lcorr_x"])
        }

        kernel_funcs_pred = {
            "IID_m": lambda theta: kernels_pred["IID_m"].forward(theta["Cv"]),
            "RBF_m": lambda theta: kernels_pred["RBF_m"].forward(theta["Cv"], length_scale=theta["lcorr_RBF"]),
            "RQD_m": lambda theta: kernels_pred["RQD_m"].forward(theta["Cv"], length_scale=theta["lcorr_RQD"],
                                                                 alpha=theta["alpha"]),
            "MAT_m": lambda theta: kernels_pred["MAT_m"].forward(theta["Cv"], length_scale=theta["lcorr_MAT"], nu=1.5),
            "EXP_m": lambda theta: kernels_pred["EXP_m"].forward(theta["Cv"], length_scale=theta["lcorr_EXP"]),
            "COS_m": lambda theta: kernels_pred["COS_m"].forward(theta["Cv"], length_scale=theta["lcorr_COS"],
                                                                 wn=theta["wn"]),
            "REF_m": lambda theta: kernels_pred["REF_m"].forward(theta["Cv"]),
            "IID_m_noise": lambda theta: kernels_pred["IID_m_noise"].forward(theta["Cv"]),
            "RBF_m_noise": lambda theta: kernels_pred["RBF_m_noise"].forward(theta["Cv"], length_scale=theta["lcorr_RBF"]),
            "RQD_m_noise": lambda theta: kernels_pred["RQD_m_noise"].forward(theta["Cv"], length_scale=theta["lcorr_RQD"],
                                                                             alpha=theta["alpha"]),
            "MAT_m_noise": lambda theta: kernels_pred["MAT_m_noise"].forward(theta["Cv"], length_scale=theta["lcorr_MAT"],
                                                                             nu=1.5),
            "EXP_m_noise": lambda theta: kernels_pred["EXP_m_noise"].forward(theta["Cv"], length_scale=theta["lcorr_EXP"]),
            "COS_m_noise": lambda theta: kernels_pred["COS_m_noise"].forward(theta["Cv"], length_scale=theta["lcorr_COS"],
                                                                             wn=theta["wn"]),
            "REF_m_noise": lambda theta: kernels_pred["REF_m_noise"].forward(theta["Cv"]),
            "IID_a": lambda theta: kernels_pred["IID_a"].forward(theta["std_model"]),
            "RBF_a": lambda theta: kernels_pred["RBF_a"].forward(theta["std_model"], length_scale=theta["lcorr_RBF"]),
            "RQD_a": lambda theta: kernels_pred["RQD_a"].forward(theta["std_model"], length_scale=theta["lcorr_RQD"],
                                                                 alpha=theta["alpha"]),
            "MAT_a": lambda theta: kernels_pred["MAT_a"].forward(theta["std_model"], length_scale=theta["lcorr_MAT"],
                                                                 nu=1.5),
            "EXP_a": lambda theta: kernels_pred["EXP_a"].forward(theta["std_model"], length_scale=theta["lcorr_EXP"]),
            "COS_a": lambda theta: kernels_pred["COS_a"].forward(theta["std_model"], length_scale=theta["lcorr_COS"],
                                                                 wn=theta["wn"]),
            "REF_a": lambda theta: kernels_pred["REF_a"].forward(theta["std_model"]),
            "IID_a_noise": lambda theta: kernels_pred["IID_a_noise"].forward(theta["std_model"]),
            "RBF_a_noise": lambda theta: kernels_pred["RBF_a_noise"].forward(theta["std_model"],
                                                                             length_scale=theta["lcorr_RBF"]),
            "RQD_a_noise": lambda theta: kernels_pred["RQD_a_noise"].forward(theta["std_model"],
                                                                             length_scale=theta["lcorr_RQD"],
                                                                             alpha=theta["alpha"]),
            "MAT_a_noise": lambda theta: kernels_pred["MAT_a_noise"].forward(theta["std_model"],
                                                                             length_scale=theta["lcorr_MAT"], nu=1.5),
            "EXP_a_noise": lambda theta: kernels_pred["EXP_a_noise"].forward(theta["std_model"],
                                                                             length_scale=theta["lcorr_EXP"]),
            "COS_a_noise": lambda theta: kernels_pred["COS_a_noise"].forward(theta["std_model"],
                                                                             length_scale=theta["lcorr_COS"],
                                                                             wn=theta["wn"]),
            "REF_a_noise": lambda theta: kernels_pred["REF_a_noise"].forward(theta["std_model"]),
            "X_dim": lambda theta: kernels_pred["X_dim"].forward(Cv_x, length_scale=theta["lcorr_x"])
        }

        # Parameters per model
        params_model = {key: list(np.append(params_unc[key], params_phys[key])) for key in models}
        n_unc = {key: len(params_unc[key]) for key in models}
        n_phys = {key: len(params_phys[key]) for key in models}
        n_theta = {key: len(params_model[key]) for key in models}


        # ===========================================================================
        # DEFINE LOGLIKELIHOODS
        # ===========================================================================
        # Define prior assuming uniform distribution for all parameters
        def prior(u, model_key):
            x = np.empty(n_theta[model_key])
            x.fill(np.nan)
            for idx_param, param in enumerate(params_model[model_key]):
                param_prior = params_priors[param]
                x[idx_param] = param_prior["low"] + (param_prior["high"] - param_prior["low"]) * u[idx_param]
            return x


        # Define loglikelihood function
        def func_loglike_2D_IID_m(th, kernel, phys_model, model_key, jitter=1e-6):

            # Assign parameters to keys
            theta = get_theta_from_input(th, params_model, model_key)

            # Assemble noise and correlation matrix
            if "std_meas" in theta.keys():
                std_noise = theta["std_meas"]
            else:
                std_noise = std_meas

            # Evaluate physical model
            y_phys = phys_model()

            y_res = y_meas - y_phys
            return iid_loglike_2D_multiplicative(y_res, y_phys, std_noise, 1.0, cov_t=theta["Cv"])


        def func_loglike_2D_IID_a(th, kernel, phys_model, model_key, jitter=1e-6):

            # Assign parameters to keys
            theta = get_theta_from_input(th, params_model, model_key)

            # Assemble noise and correlation matrix
            if "std_meas" in theta.keys():
                std_noise = theta["std_meas"]
            else:
                std_noise = std_meas

            # Evaluate physical model
            y_phys = phys_model()

            y_res = y_meas - y_phys
            return iid_loglike_2D_additive(y_res, std_noise, 1.0, std_t=theta["std_model"])


        def func_loglikelihood_m(th, kernel, phys_model, model_key, jitter=1e-6):

            # Assign parameters to keys
            theta = get_theta_from_input(th, params_model, model_key)

            # Assemble noise and correlation matrix
            if "std_meas" in theta.keys():
                std_noise = theta["std_meas"]
            else:
                std_noise = std_meas

            # Cx is the correlation between sensors
            Cx_0, Cx_1 = inv_cov_vec_1D(np.array(sensor_x), theta["lcorr_x"], np.repeat(Cv_x, Nx))
            Cx = [Cx_0, Cx_1]

            # Ct is the correlation between measurements
            Ct = np.linalg.inv(kernel(theta) + np.diag(np.ones(Nt) * jitter))

            # Evaluate physical model

            # TODO: Check if this is correct. Why are we transposing the measurements and
            #  model predictions? (probably to accomodate `chol_loglike_2D`)
            y_phys = np.transpose(phys_model())
            y_res = np.transpose(y_meas) - y_phys

            L_left = chol_loglike_2D(y_res[:, :Nx], Ct, Cx, std_noise, y_model=y_phys[:, :Nx])
            L_right = chol_loglike_2D(y_res[:, Nx:], Ct, Cx, std_noise, y_model=y_phys[:, Nx:])

            # # Reference solution
            # k_cov_t = kernel(theta)# + np.diag(np.ones(Nt) * jitter)
            # inv_k_cov_x_l = np.diag(Cx_1, k = -1)
            # inv_k_cov_x_u = np.diag(Cx_1, k = 1)
            # inv_k_cov_x = inv_k_cov_x_l + inv_k_cov_x_u
            # np.fill_diagonal(inv_k_cov_x, Cx_0)
            # k_cov_x = np.linalg.inv(inv_k_cov_x)
            #
            # k_cov_mx = np.kron(k_cov_x, k_cov_t)
            # e_cov_mx = np.diag(std_vec ** 2)
            #
            # # Evaluate reference for left and right
            # y_phys_diag_l = np.diag(y_phys[:, :Nx].ravel())
            # kph_cov_mx_l = np.matmul(np.matmul(y_phys_diag_l, k_cov_mx), y_phys_diag_l)
            # cov_l = kph_cov_mx_l + e_cov_mx
            # loglike_ref_l = stats.multivariate_normal.logpdf(y_res[:, :Nx].ravel(), cov=cov_l)
            #
            # y_phys_diag_r = np.diag(y_phys[:, Nx:].ravel())
            # kph_cov_mx_r = np.matmul(np.matmul(y_phys_diag_r, k_cov_mx), y_phys_diag_r)
            # cov_r = kph_cov_mx_r + e_cov_mx
            # loglike_ref_r = stats.multivariate_normal.logpdf(y_res[:, Nx:].ravel(), cov=cov_r)
            #
            # print(L_left)
            # print(loglike_ref_l)
            # print(L_right)
            # print(loglike_ref_r)

            return L_left + L_right


        def func_loglikelihood_a(th, kernel, phys_model, model_key, jitter=1e-6):

            # Assign parameters to keys
            theta = get_theta_from_input(th, params_model, model_key)

            # Assemble noise and correlation matrix
            if "std_meas" in theta.keys():
                std_noise = theta["std_meas"]
            else:
                std_noise = std_meas

            # Cx is the correlation between sensors
            Cx_0, Cx_1 = inv_cov_vec_1D(np.array(sensor_x), theta["lcorr_x"], np.repeat(Cv_x, Nx))
            Cx = [Cx_0, Cx_1]

            # Ct is the correlation between measurements
            Ct = kernel(theta) + np.diag(np.ones(Nt) * jitter)

            # Here we probably don't have to take the transpose since the dimensions are irrelevant for the
            # kronecker loglikekihood
            y_phys = phys_model()
            y_res = y_meas - y_phys

            L_left = kron_loglike_2D(y_res[:Nx, :], Cx, Ct, std_noise)
            L_right = kron_loglike_2D(y_res[Nx:, :], Cx, Ct, std_noise)

            # # Reference solution
            return L_left + L_right


        def func_post_sample_m(th, kernel_x, kernel_t, phys_model, model_key, samples=1):

            # Assign parameters to keys
            theta = get_theta_from_input(th, params_model, model_key)

            # Assemble noise and correlation matrix
            if "std_meas" in theta.keys():
                std_noise = theta["std_meas"]
            else:
                std_noise = std_meas

            # Evaluate physical model
            y_phys = phys_model()
            Nt = y_phys.shape[1]

            y_left = torch.tensor(np.ravel(y_phys[:Nx, :]))
            y_right = torch.tensor(np.ravel(y_phys[Nx:, :]))

            Y_left = torch.tensor(np.diag(y_left))
            Y_right = torch.tensor(np.diag(y_right))

            # Measurement and modeling uncertainty
            e_cov_mx = np.diag(np.repeat(std_noise ** 2, Nx * Nt))
            k_cov_x = kernel_x(theta)
            k_cov_t = kernel_t(theta)
            k_cov_xt = np.kron(k_cov_x, k_cov_t)

            cov_left = np.matmul(Y_left, np.matmul(k_cov_xt, Y_left)) + e_cov_mx
            cov_right = np.matmul(Y_right, np.matmul(k_cov_xt, Y_right)) + e_cov_mx

            mvn_left = MultivariateNormal(y_left, cov_left)
            mvn_right = MultivariateNormal(y_right, cov_right)

            post_samples_l = mvn_left.sample().detach().cpu().numpy()
            post_samples_r = mvn_right.sample().detach().cpu().numpy()

            # Reshape resulting samples into array
            post_samples_l = np.reshape(post_samples_l, (Nx, -1))
            post_samples_r = np.reshape(post_samples_r, (Nx, -1))
            return post_samples_l, post_samples_r


        def func_post_sample_iid_m(th, dummy_1, dummy_2, phys_model, model_key):

            # Assign parameters to keys
            theta = get_theta_from_input(th, params_model, model_key)

            # Assemble noise and correlation matrix
            if "std_meas" in theta.keys():
                std_noise = theta["std_meas"]
            else:
                std_noise = std_meas

            # Evaluate physical model
            y_phys = phys_model()
            Nt = y_phys.shape[1]

            y_left = np.ravel(y_phys[:Nx, :])
            y_right = np.ravel(y_phys[Nx:, :])

            # Measurement and modeling uncertainty
            e_meas_vec = np.repeat(std_noise, Nx * Nt)
            k_model_vec = np.repeat(theta["Cv"], Nx * Nt)

            std_left = np.sqrt(y_left ** 2 * k_model_vec ** 2 + e_meas_vec ** 2)
            std_right = np.sqrt(y_right ** 2 * k_model_vec ** 2 + e_meas_vec ** 2)

            post_samples_l = stats.norm.rvs(loc=y_left, scale=std_left)
            post_samples_r = stats.norm.rvs(loc=y_right, scale=std_right)

            # Reshape resulting samples into array
            post_samples_l = np.reshape(post_samples_l, (Nx, -1))
            post_samples_r = np.reshape(post_samples_r, (Nx, -1))

            return post_samples_l, post_samples_r


        def func_post_sample_iid_a(th, dummy_1, dummy_2, phys_model, model_key):

            # Assign parameters to keys
            theta = get_theta_from_input(th, params_model, model_key)

            # Assemble noise and correlation matrix
            if "std_meas" in theta.keys():
                std_noise = theta["std_meas"]
            else:
                std_noise = std_meas

            # Evaluate physical model
            y_phys = phys_model()

            y_left = np.ravel(y_phys[:Nx, :])
            y_right = np.ravel(y_phys[Nx:, :])
            Nt = y_phys.shape[1]

            # Measurement and modeling uncertainty
            e_meas_vec = np.repeat(std_noise, Nx * Nt)
            k_model_vec = np.repeat(theta["std_model"], Nx * Nt)

            std_left = np.sqrt(k_model_vec ** 2 + e_meas_vec ** 2)
            std_right = np.sqrt(k_model_vec ** 2 + e_meas_vec ** 2)

            post_samples_l = stats.norm.rvs(loc=y_left, scale=std_left)
            post_samples_r = stats.norm.rvs(loc=y_right, scale=std_right)

            # Reshape resulting samples into array
            post_samples_l = np.reshape(post_samples_l, (Nx, -1))
            post_samples_r = np.reshape(post_samples_r, (Nx, -1))

            return post_samples_l, post_samples_r


        def func_post_sample_a(th, kernel_x, kernel_t, phys_model, model_key, samples=1):

            # Assign parameters to keys
            theta = get_theta_from_input(th, params_model, model_key)

            # Assemble noise and correlation matrix
            if "std_meas" in theta.keys():
                std_noise = theta["std_meas"]
            else:
                std_noise = std_meas

            # Evaluate physical model
            y_phys = phys_model()
            Nt = y_phys.shape[1]

            y_left = torch.tensor(np.ravel(y_phys[:Nx, :]))
            y_right = torch.tensor(np.ravel(y_phys[Nx:, :]))

            # Measurement and modeling uncertainty
            e_cov_mx = np.diag(np.repeat(std_noise ** 2, Nx * Nt))
            k_cov_x = kernel_x(theta)
            k_cov_t = kernel_t(theta)
            k_cov_xt = np.kron(k_cov_x, k_cov_t)

            cov_left = torch.tensor(k_cov_xt + e_cov_mx)
            cov_right = torch.tensor(k_cov_xt + e_cov_mx)

            mvn_left = MultivariateNormal(y_left, cov_left)
            mvn_right = MultivariateNormal(y_right, cov_right)

            post_samples_l = mvn_left.sample().detach().cpu().numpy()
            post_samples_r = mvn_right.sample().detach().cpu().numpy()

            # Reshape resulting samples into array
            post_samples_l = np.reshape(post_samples_l, (Nx, -1))
            post_samples_r = np.reshape(post_samples_r, (Nx, -1))

            return post_samples_l, post_samples_r


        # ============================================================================
        # Assemble lists of functions
        # ============================================================================
        loglike_list = {
            "IID_m": func_loglike_2D_IID_m,
            "RBF_m": func_loglikelihood_m,
            "MAT_m": func_loglikelihood_m,
            "EXP_m": func_loglikelihood_m,
            "IID_m_noise": func_loglike_2D_IID_m,
            "RBF_m_noise": func_loglikelihood_m,
            "MAT_m_noise": func_loglikelihood_m,
            "EXP_m_noise": func_loglikelihood_m,
            "IID_a": func_loglike_2D_IID_a,
            "RBF_a": func_loglikelihood_a,
            "MAT_a": func_loglikelihood_a,
            "EXP_a": func_loglikelihood_a,
            "IID_a_noise": func_loglike_2D_IID_a,
            "RBF_a_noise": func_loglikelihood_a,
            "MAT_a_noise": func_loglikelihood_a,
            "EXP_a_noise": func_loglikelihood_a,
        }

        # TODO: THe posterior sampling functions are not actually implemented
        post_pred_list = {
            "IID_m": func_post_sample_iid_m,
            "RBF_m": func_post_sample_m,
            "MAT_m": func_post_sample_m,
            "EXP_m": func_post_sample_m,
            "REF_m": func_post_sample_iid_m,
            "IID_m_noise": func_post_sample_iid_m,
            "RBF_m_noise": func_post_sample_m,
            "MAT_m_noise": func_post_sample_m,
            "EXP_m_noise": func_post_sample_m,
            "REF_m_noise": func_post_sample_iid_m,
            "IID_a": func_post_sample_iid_a,
            "RBF_a": func_post_sample_a,
            "MAT_a": func_post_sample_a,
            "EXP_a": func_post_sample_a,
            "REF_a": func_post_sample_iid_a,
            "IID_a_noise": func_post_sample_iid_a,
            "RBF_a_noise": func_post_sample_a,
            "MAT_a_noise": func_post_sample_a,
            "EXP_a_noise": func_post_sample_a,
            "REF_a_noise": func_post_sample_iid_a,
        }

        # Assemble dictionaries of prior, loglikelihood and sampling functions.
        prior_funcs = {}
        loglikelihood_funcs = {}
        data_generation_funcs = {}
        post_pred_funcs = {}

        for key in models:
            prior_funcs[key] = lambda u, key=key: prior(u, key)
            loglikelihood_funcs[key] = lambda theta, key=key: loglike_list[key](theta, kernel_funcs_infer[key],
                                                                                phys_model_infer, key)
            data_generation_funcs[key] = lambda theta, key=key: post_pred_list[key](theta, kernel_funcs_infer["X_dim"],
                                                                                    kernel_funcs_infer[key],
                                                                                    phys_model_infer, key)
            post_pred_funcs[key] = lambda theta, key=key: post_pred_list[key](theta, kernel_funcs_pred["X_dim"],
                                                                              kernel_funcs_pred[key], phys_model_pred, key)

        # ============================================================================
        # GENERATE SYNTHETIC MEASUREMENTS
        # ============================================================================

        theta_gt_list = [
            dict_gt[param_i] for idx_param, param_i in enumerate(params_model[gt_model])
        ]

        # True mean system response
        if save_meas_flag == 1:
            y_l, y_r = data_generation_funcs[gt_model](np.array(theta_gt_list))
            y_meas = np.vstack((y_l, y_r))
            save_post_pred(meas_save_path, gt_model + f"_Nx_{Nx}_Nt_{Nt}_iter{iter_i}" + key + ".pickle", y_meas)
        else:
            y_meas = load_pickle(meas_save_path, gt_model + f"_Nx_{Nx}_Nt_{Nt}_iter{iter_i}" + key + ".pickle")
            y_l = y_meas[:Nx, :]
            y_r = y_meas[Nx:, :]

        # ============================================================================
        # INFERENCE
        #
        # Walker initial positions are obtained by uniform sampling from the prior
        # transforms.
        # ============================================================================

        # Assemble lists of parameters
        K_labels_list = [
            r"$K_{r," + str(i + 1) + "}$" for i in range(4)
        ]

        model_titles_list = [model_latex_labels[_key] for _key in models]

        # Inference with emcee for each statistical model.
        stat_models = {}
        latex_tables = dict.fromkeys(model_titles_list)

        sampling_kwargs = {
            "nlive": nlive,
            "sample": sample,
            "bound": bound,
        }

        for ii, key in enumerate(models_run):
            print("==============================================================")
            print(f"Run {ii + 1} of {len(models_run)}, model: {key}")
            print("==============================================================")

            # New runs
            if load_data_flag == 0:

                stat_models[key] = DynestyParameterEstimator(
                    log_likelihood=loglikelihood_funcs[key],
                    prior_transform=prior_funcs[key],
                    ndim=n_theta[key],
                    estimation_method=estimation_method,
                    kwargs=sampling_kwargs,
                )

                # Estimate parameters
                stat_models[key].estimate_parameters(maxcall=maxcall)

                # Save stat model
                if save_model_flag == 1:
                    save_model(
                        data_save_path, gt_model + f"_Nx_{Nx}_Nt_{Nt}_iter{iter_i}" + key + ".pickle",
                        stat_models[key],
                        timestamp=False,
                    )

            # Load old runs
            elif load_data_flag == 1:
                stat_models[key] = load_pickle(data_save_path, gt_model + f"_Nx_{Nx}_Nt_{Nt}_iter{iter_i}" + key + ".pickle")

            # Get summary latex table
            # latex_tables[model_titles_list[ii]] = summary(stat_models[ii])

            # Plot prior and posterior and print summary
            range_min = prior_funcs[key](np.zeros(n_theta[key]) + 0.0001)
            range_max = prior_funcs[key](np.ones(n_theta[key]) - 0.0001)
            posterior_plot_range = np.vstack([range_min, range_max])
            if plot_res_flag == 1:
                params_labels = [params_latex_labels[param] for param in params_model[key]]
                params_ground_truth = [dict_gt[param] for param in params_model[key]]
                stiffness_transform = [params_transforms[param] for param in params_model[key]]
                fig, ax = plot_posterior(
                    stat_models[key],
                    params_labels,
                    key,
                    plot_range=posterior_plot_range,
                    ground_truth=params_ground_truth,
                    transform=stiffness_transform
                )
                fig.savefig(
                    str(fig_post_save_path) + "\\posterior" + truck_path + "_"+ gt_model + f"_Nx_{Nx}_Nt_{Nt}_iter{iter_i}" + meas_type + "_" + key
                )
                plt.show()
        stat_models_list.append(stat_models)

        # ==========================================================================
        # GET POINT ESTIMATES
        # ==========================================================================
        # Get parameter MAP estimates for ground truth model
        param_mean, param_median, param_MAP = get_parameter_estimates(stat_models[gt_model])
        param_estimates_list.append(param_MAP)

        #             # stat_models[key].summary()
        #             # fig, ax = dynesty_run_plot(stat_models[key])
        #             # fig, ax = dynesty_trace_plot(stat_models[key])

        # ==========================================================================
        # MODEL SELECTION AND BAYES FACTORS
        #
        # * Models are considered a-priori equally probable
        # ==========================================================================

        # Create latex table from model selection results
        model_selection_list = [stat_models[key] for key in models_evidence]
        model_selection_latex = summary_model_selection(
            model_selection_list, model_names=models_evidence, tablefmt="latex_raw"
        )
        model_selection_results = model_selection(model_selection_list)
        model_selection_labels = ["log($\mathcal{Z}$)", "log($\mathcal{Z}_{data}$)", "log($\mathcal{Z}_{occam}$)",
                                  r"$p_i$, "r"$K$",
                                  "Interpretation"]

        model_selection_results_list.append(model_selection_results)
        model_selection_latex_list.append(model_selection_latex)

        # ===========================================================================
        # Plot credible intervals
        # ===========================================================================

        # Resample, otherwise the large x support makes the CI calculation fail when
        # the posteriors tend to a delta
        reset_global_default()

        for group in model_groups.keys():
            params_group = params_plot[group]
            params_labels = [params_latex_labels[param] for param in params_group]
            ci_range_min = [params_priors[p_i]["low"] for p_i in params_group]
            ci_range_max = [params_priors[p_i]["high"] for p_i in params_group]
            ci_plot_range = np.transpose(np.vstack([ci_range_min, ci_range_max]))

            resample_plot = []
            print("Credible interval comparison plots")
            for idx_key, key in enumerate(model_groups[group]):
                print(f"Model: {key}")

                # Get index of parameters
                idx_params = get_parameter_idx(params_group, params_model[key])
                resample = np.transpose(dynesty_resample(stat_models[key], n_samples_ci)[:, idx_params])
                resample_plot.append(resample)

            if plot_res_flag:
                row_names = model_groups[group]
                fig, ax, crdist = plot_credible_intervals(resample_plot, row_names, params_group, col_labels=params_labels,
                                                          plot_range=ci_plot_range)
                fig.savefig(
                    str(fig_post_save_path) + "\\all_posteriors_" + truck_path + "_" + group + "_"
                    + gt_model + f"_Nx_{Nx}_Nt_{Nt}_iter{iter_i}" + meas_type
                )
                plt.show()

        # ===========================================================================
        # Posterior predictive for sensor where inference was performed
        # ===========================================================================

        print("Posterior predictives")
        # Plot parameters
        reset_global_default()
        update_plot_params(paramsPostPred)
        plot_range = np.array([[-20.0, 40.0], [-40.0, 60.0]], dtype=object)

        # Reshape data into a more convenient shape
        y_true = np.zeros([2, Nx, Nt])
        y_true[0, :, :] = y_meas[:Nx, :]
        y_true[1, :, :] = y_meas[Nx:, :]

        # True response at all peaks for all sensors
        y_true_peaks = np.zeros([2, Nx, Nx])
        y_true_peaks[0, :, :] = y_meas[:Nx, idx_x_pred]
        y_true_peaks[1, :, :] = y_meas[Nx:, idx_x_pred]

        # Equally weighted resampling from the statistical models
        resample_post = {}
        samples_post_pred = {}
        samples_noise = {}
        for idx_key, key in enumerate(models_run):
            print(f"Resampling from {key}")
            resample_post[key] = dynesty_resample(stat_models[key], n_samples_post_pred)
            func_noise = lambda theta, key=key: func_sample_noise(theta, params_model, key, std_meas)

            if post_pred_flag == 1:

                # TODO: RESHAPE y_meas so its shape is compatible with the plotting of multiple sensors
                fname = str(
                    fig_valid_save_path) + "\\" + key + "_from_" + gt_model + f"_Nx_{Nx}_Nt_{Nt}_iter{iter_i}" + meas_type + "_" + truck_path + "_"

                # Draw samples from noise and posterior predictive
                samples_noise[key] = func_noise(resample_post[key])
                # Call to `sample_post_pred`
                if save_results_flag == 1:
                    print(f"Sampling posterior predictive for {key}")
                    samples_post_pred[key] = sample_post_pred(post_pred_funcs[key], resample_post[key])
                    save_post_pred(results_save_path, gt_model + f"_Nx_{Nx}_Nt_{Nt}_iter{iter_i}" + key + ".pickle", samples_post_pred[key])
                else:
                    samples_post_pred[key] = load_pickle(results_save_path, gt_model + f"_Nx_{Nx}_Nt_{Nt}_iter{iter_i}" + key + ".pickle")

                print("Call to `plot_post_red`")
                fig, ax, metrics = plot_post_pred(
                    samples_post_pred[key],
                    samples_noise[key],
                    x_pred,
                    y_true_peaks,
                    fname,
                    std_meas=std_meas,
                    plot_range=plot_range,
                    plot_title=model_latex_labels[key],
                    subplot_titles=fnames,
                    prob_mass=prob_mass
                )
                plt.show()

            else:
                if load_results_flag == 1:
                    samples_post_pred[key] = load_pickle(results_save_path, gt_model + f"_Nx_{Nx}_Nt_{Nt}_iter{iter_i}" + key + ".pickle")

        # ============================================================================
        # Plot the peak distributions of stresses per model and sensor
        # Get data: samples_post[nsamples, lane, model, node]
        # Input to plot_credible_intervals has shape (nrow, ncol, nsample)
        # ============================================================================

        if plot_samples_peak_flag:
            print("Sampling and plotting peaks")
            samples_peak = np.zeros((N_models, n_samples_post_pred, N_out, Nx))
            for idx_key, key in enumerate(models_run):
                samples_post = samples_post_pred[key]
                arr_mean = np.mean(samples_post, axis=0)
                idx_peak = np.argmax(arr_mean, axis=-1)
                mean_error = np.abs(y_true_peaks - arr_mean)

                # Get the size of the sampling function output
                samples_post_shape = np.shape(samples_post)

                # Metrics at peak
                peak_error = np.zeros((N_out, Nx))
                meas_peak = np.zeros((N_out, Nx))
                for i in range(N_out):
                    for j in range(Nx):
                        meas_peak[i, j] = y_true_peaks[i, j, idx_peak[i, j]]
                        peak_error[i, j] = mean_error[i, j, idx_peak[i, j]]
                        samples_peak[idx_key, :, i, j] = samples_post[:, i, j, idx_peak[i, j]]

                # Assemble into arrays with correct shape for plotting
                samples_peak_left = np.swapaxes(samples_peak[:, :, 0, :], 1, 2)
                samples_peak_right = np.swapaxes(samples_peak[:, :, 1, :], 1, 2)

                samples_peak_left = np.swapaxes(samples_peak_left, 0, 1)
                samples_peak_right = np.swapaxes(samples_peak_right, 0, 1)

            # fig_l, ax_l, cr_left = plot_credible_intervals(
            #     samples_peak_left,
            #     fnames,
            #     model_titles_list,
            #     # col_labels=fnames,
            #     # ground_truth=meas_peak[0],
            #     cr_prob_mass=prob_mass,
            # )
            # fig_l.savefig(
            #     str(fig_valid_save_path) + "\\" + "ci_l_" + key + "_from_" + gt_model + f"_Nx_{Nx}_Nt_{Nt}_iter{iter_i}" + meas_type + "_" + truck_path + "_" + str(
            #         nnode)
            # )
            #
            # fig_r, ax_r, cr_right = plot_credible_intervals(
            #     samples_peak_right,
            #     fnames,
            #     model_titles_list,
            #     # col_labels=fnames,
            #     # ground_truth=meas_peak[1],
            #     cr_prob_mass=prob_mass,
            # )
            # fig_r.savefig(
            #     str(fig_valid_save_path) + "\\" + "ci_r_" + key + "_from_" + gt_model + f"_Nx_{Nx}_Nt_{Nt}_iter{iter_i}" + meas_type + "_" + truck_path + "_" + str(
            #         nnode))

# ============================================================================================
# PLOT MAP ESTIMATES
# ============================================================================================
params_unc_gt = [dict_gt[item] for item in params_unc[gt_model]]
param_estimates_reshaped = np.reshape(param_estimates_list, (Niter, len(Nsens_per_span), -1))
param_estimates_averaged = param_estimates_reshaped.mean(axis=0)
param_estimates_rel_err = np.abs((param_estimates_averaged - params_unc_gt) / params_unc_gt)
param_plot_lstyles = ["solid", "dashed", "dotted", "dashdot"]

# Plot
plt.figure()
for idx_p, param_i_name in enumerate(params_unc[gt_model]):
    plt.plot(5 * Nsens_per_span, param_estimates_rel_err[:, idx_p], label=params_latex_labels[param_i_name], linestyle=param_plot_lstyles[idx_p], c="black")
plt.grid()
plt.ylim(0.0, 1.0)
plt.legend(fontsize=22)
plt.title(f"Model: {model_latex_labels[gt_model]}", fontsize=28)
plt.xlabel("Grid size", fontsize=24)
plt.ylabel("MAP estimate relative error", fontsize=24)
plt.show()
# plt.savefig(str(fig_post_save_path) + "\\synthetic_" + gt_model, bbox_inches="tight")

# ============================================================================================
# PLOT EVIDENCE
# ============================================================================================

# model_selection_results_list = []
# model_selection_latex_list = []
# stat_models_list = []

# Loop over the model selection results and assemble them in a form that
# is appropriate for plotting and tabulating
dict_model_evidence = {key: [] for key in models_evidence}

# Each `list_i` contains the model selection results for all models at a specified
# number of points for one iteration
for list_i in model_selection_results_list:
    for idx_model, model_i in enumerate(models_evidence):
        print(f"model = {model_i}, Z = {list_i[idx_model][0]}")
        dict_model_evidence[model_i].append(list_i[idx_model][0])

# Reshape for plotting and tabulating
dict_model_evidence_reshaped = {key: [] for key in models_evidence}
dict_model_evidence_mean = np.zeros((len(models_evidence), len(Nsens_per_span)))
for idx_model, model_i in enumerate(models_evidence):
    dict_model_evidence_reshaped[model_i] = np.reshape(dict_model_evidence[model_i], (Niter, -1))
    dict_model_evidence_mean[idx_model] = dict_model_evidence_reshaped[model_i].mean(axis=0)

# Copy to dataframe and add to clipboard
df = pd.DataFrame(dict_model_evidence_mean)
# df.to_clipboard(index=False, header=False)

plt.figure()
for model_i in dict_model_evidence.keys():
    plt.plot(dict_model_evidence_reshaped[model_i].mean(axis=0), label=model_latex_labels[model_i])
plt.grid()
plt.legend()
plt.show()